{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Seed Document for Analysis\n",
    "=====================================\n",
    "\n",
    "This notebook outlines a tutorial for including data for the Web Archive Collections.  This includes\n",
    "- Descriptive Data for the collection itself\n",
    "- Selected Description of Most Interesting Seeds\n",
    "- Time-series analysis of the main domains through Crawl-viz (https://github.com/web-archive-group/WALK-CrawlVis)\n",
    "- Including some field notes to help others get a \"feel\" for what's in the collection.\n",
    "- Possibly utilizing some annotation software like Hypothes.is to take advantage of crowd sourcing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the Archive\n",
    "=====================\n",
    "\n",
    "The first step is to view the archive at archive-it.org.  For this example, I am going to use the [Canadian Political Parties and Political Interest Groups Archive](https://archive-it.org/collections/227). Reading through the archive, what websites are archived, who owns it and so on is the first main step.  Maybe jot down some notes on these questions:\n",
    "\n",
    "- What kinds of sites are being collected? (Political parties & interest groups)\n",
    "- How long have they been collecting data? (for over 10 years)\n",
    "- What qualitative evaluations can you make? (In general, marginal parties like The Cosmopolitan Party are treated as having equal importance as established parties such as the Liberals, Conservatives & NDP.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the Crawl-Viz\n",
    "=======================\n",
    "\n",
    "Each crawl has been analyse in a time series via the [Crawl-Viz data site](https://web-archive-group.github.io/WALK-CrawlVis/crawl-sites). [CPP's crawl viz](https://web-archive-group.github.io/WALK-CrawlVis/crawl-sites/TORONTO_Canadian_Political_Parties-urls.html) is very interesting. For example, one might expect that established parties would have more to say than marginal parties. In fact, the busiest web site appears to be [equalvoice.ca](http://www.equalvoice.ca), an advocacy group committed to including more women in political life. Nonetheless, additional notes about the crawl-viz would be very helpful.  Jot them down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter Data into a NoSql Database\n",
    "\n",
    "NoSql is all the rage these days. Not that that really means that much. Sql or *structured query language* was the standard for everything not very long ago. The reason for this is that computer processing had a hard time processing lots of data, so we spent a lot of time trying to keep everything organized so it was easy to get at later. NoSQL does not use SQL. That sounds fancy, but one of the most common ways to store data is to apply \"key-value pairs\" which, if you know anything about programming, is the exact same thing that a Dictionary in Python or an Object in Javascript does.  It's also the way that JSON, a popular alternative to XML, works.  For instance:\n",
    "<pre>\n",
    "{\"key\" : \"value\" }\n",
    "</pre>\n",
    "\n",
    "There!  I just did nosql.  So exciting eh?  Well, there's more to it than this of course. One advantage to this approach is its flexibility.  Instead of conforming to a specific schema or structure, you can just put the data in a store and make things up as you go along. The code below does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TinyDB is a very simple NoSQL database used for small projects.  It's not ideal for large projects\n",
    "# because it is file-based, meaning only one person can change the database at a time.\n",
    "#\n",
    "# We are also importing the time library so that we can include some timestamps as part of the work.\n",
    "\n",
    "import tinydb as tdb\n",
    "import time\n",
    "import networkx\n",
    "import gdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up database\n",
    "db = tdb.TinyDB(\"./data/WALK.json\")\n",
    "\n",
    "# db is now what our database is called.  We can also create some tables\n",
    "\n",
    "collections = db.table(\"collections\")  # the table for describing an overall collection\n",
    "collections_backup = db.table(\"collections_backup\") # a table for backing things up whenever we mess up\n",
    "\n",
    "seed_main = db.table(\"seed_main\") # in each collection, there are a number of websites we might want to describe.\n",
    "seed_backup = db.table(\"seed_backup\") # again, a backup in case we lose information.\n",
    "\n",
    "Seed = tdb.Query()  # Seed will be the query object we will use (instead of having to write Query() all the time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# !!!!!!!!!!!!!  USE WITH CARE!  Will delete all tables from the database !!!!!!!!!!!!!!!!\n",
    "\n",
    "################# seed_main.purge()\n",
    "################# seed_backup.purge()\n",
    "################# collections.purge()\n",
    "################# collections_backup.purge()\n",
    "################# db.purge_tables()\n",
    "################# default = db.table(\"_default\")\n",
    "print (db.tables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe the collection and add to the Database\n",
    "\n",
    "A tinyDB accepts python objects (or dictionaries) in key-value pairs as data. Therefore, if we want to add information into the database, we must make it conform to python dictionary format: { 'key' : 'value\" }.  In python, we can do this one of two ways.  \n",
    "\n",
    "1. Write out the whole dictionary each time (next slide) OR\n",
    "2. add data to the object after the fact.  For example, \n",
    "   <pre> description['key'] = value \n",
    "   </pre>\n",
    "   will put \"value\" in the dictionary that can be accessed by searching\n",
    "   \"key.\"\n",
    "   \n",
    "### Troubleshooting\n",
    "\n",
    "#### Invalid Syntax:\n",
    "Check for:\n",
    "    1. commas after each selection\n",
    "    2. all quotes are completed with the same format.\n",
    "       ( NOTE: in python, single quotes assume everything inside are literally correct.\n",
    "       Double quotes accept programming instructions, unless otherwise escaped.\n",
    "       I suggest using single quotes because '&' will properly give you & while \"&\" might\n",
    "       think & is doing something special.\n",
    "    3. Keys are encased in quotes. 'key'\n",
    "    4. Values are encased in quotes unless they are number values.\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "description = {\n",
    "    ## collection_title and WALK_collection_folder are used to decide\n",
    "    ## whether to insert a new item or to update an old one ...\n",
    "    \n",
    "    'collection_title' : 'Canadian Political Parties and Political Interest Groups',\n",
    "    \n",
    "    'WALK_collection_folder' : 'TORONTO_Canadian_Political_Parties',\n",
    "    \n",
    "    ## If you accidentally update the wrong item, you can retrieve the old value from the \n",
    "    ## collections_backup table.\n",
    "    \n",
    "    \n",
    "    # How does the Library/Archives describe the archive?\n",
    "    'institutional_description' : '''\n",
    "    \n",
    "    Canadian Political Parties and Political Interest Groups will archive the websites of all \n",
    "    the national Canadian political parties, and a number of special interest groups across \n",
    "    the political spectrum.\n",
    "    \n",
    "    ''',\n",
    "    \n",
    "    # In your own words, how do you describe the collection\n",
    "    'WALK_description' : '''\n",
    "    \n",
    "    Contains the web archives for the main parties (Liberal, Conservatives, NDP, Bloc, Green) but \n",
    "    also a wide range marginal parties (Cosmopolitan Party, Canadian Action, Christian Heritage and\n",
    "    so on).  The \"special interest groups\" include the David Suzuki Foundation (an environmental \n",
    "    advocacy group) and fairvote.ca (advocacy for changing the electoral system).\n",
    "    \n",
    "    ''',\n",
    "    \n",
    "    # What file did you use to view the viz-link?\n",
    "    'crawl_viz_link_file' : 'TORONTO_Canadian_Political_Parties-urls.html',\n",
    "    \n",
    "    'crawl_viz_description' : '''\n",
    "    \n",
    "    - Between March 06 and January 07 and then again between July 09 & November 09, Policy Alternatives had the\n",
    "    largest amount of activity.\n",
    "    - A rise in activity for equalvoice.ca (advocacy for women in political leadership) between December 09 and \n",
    "    November 2011.\n",
    "    - Of the major parties, the Liberal Party of Canada and the Green Party had the most activity.\n",
    "    \n",
    "    ''',\n",
    "    \n",
    "    'gephi_file_name' : '',\n",
    "    \n",
    "    'gephi_description': '''\n",
    "    ''',\n",
    "    \n",
    "    'gephi_avg_degree': 0,\n",
    "    \n",
    "    'gephi_avg_weighted_degree' : 0,\n",
    "    \n",
    "    'gephi_clustering_coefficient' : 0,\n",
    "    \n",
    "    'something wrong' : False,\n",
    "    \n",
    "    ##  You can add additional items here using the format \n",
    "    ##  'META_DATA_TAG' : 'DATA_VALUES',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "exists = collections.search(Seed.collection_title==description['collection_title']) and collections.search(Seed.WALK_collection_folder==description['WALK_collection_folder'])\n",
    "description['TIMESTAMP'] = time.time() #create a timestamp\n",
    "if exists:\n",
    "    # print(collections.search(Seed.collection_title.exists()))\n",
    "    el = collections.get(Seed.collection_title==description['collection_title'])\n",
    "    il = collections.get(Seed.WALK_collection_folder==description['WALK_collection_folder'])\n",
    "    if el.eid == il.eid:\n",
    "        collections.update(description, eids=[el.eid])\n",
    "        collections_backup.insert(description)\n",
    "    print (\"updated \"  + description['collection_title'] + \".\\n\")\n",
    "    print (\"Previous insert added to backup log.\\n\")\n",
    "    backup = max([(x['collection_title'], x['TIMESTAMP'], x.eid) for x in collections_backup.search(Seed.TIMESTAMP > 0)])\n",
    "    print ('title: \"' + backup[0] + \"\\ntimestamp: \" + str(backup[1]) + \"\\neid (aka id): \" + str(backup[2]))\n",
    "        \n",
    "else:\n",
    "    collections.insert(description)\n",
    "    print (\"inserted!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More detailed description of collection contents\n",
    "\n",
    "Within each Collection, there may be one or more seeds that are worth an additional look.  I propose using the seeds table for this.  Again, you can include whatever additional information you think is relevant by providing an additional key: value pair.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'TIMESTAMP': 1488049265.426975, 'description': '\\n            Also called the \"Progressive Nationalist Party\", it is a \"progressive and environment protection oriented\\n            political party\" that seeks the \"political, economic and cultural assimilation of Canada, into the\\n            United States, under the _Security and Prosperity Partnership_ (SPP).\"\\n        ', 'latest_crawl': '2012-11-03', 'url': 'http://agoracosmopolite.com', 'videos': 0, 'times_captured': 59, 'seed_name': 'Cosmopolitan Party of Canada', 'WALK_collection_folder': 'TORONTO_Canadian_Political_Parties', 'collection_title': 'Canadian Political Parties and Political Interest Groups', 'first_crawl': '2005-10-04'}]\n",
      "updated Canadian Political Parties and Political Interest Groups.\n",
      "\n",
      "Previous insert added to backup log.\n",
      "\n",
      "title: \"Canadian Political Parties and Political Interest Groups\n",
      "timestamp: 1488049284.002535\n",
      "eid (aka id): 2\n"
     ]
    }
   ],
   "source": [
    "seed = {\n",
    "        \"collection_title\" : \"Canadian Political Parties and Political Interest Groups\",\n",
    "        \"WALK_collection_folder\" : \"TORONTO_Canadian_Political_Parties\",\n",
    "        \"seed_name\" : \"Cosmopolitan Party of Canada\",\n",
    "        \"first_crawl\" : \"2005-10-04\",\n",
    "        \"latest_crawl\" : \"2012-11-03\",\n",
    "        \"times_captured\": 59,\n",
    "        \"videos\" : 0,\n",
    "        \"url\" : \"http://agoracosmopolite.com\",\n",
    "        \"description\" : '''\n",
    "            Also called the \"Progressive Nationalist Party\", it is a \"progressive and environment protection oriented\n",
    "            political party\" that seeks the \"political, economic and cultural assimilation of Canada, into the\n",
    "            United States, under the _Security and Prosperity Partnership_ (SPP).\"\n",
    "        '''\n",
    "    }\n",
    "\n",
    "seed_exists = ((seed_main.search(Seed.collection_title==seed['collection_title']) or seed_main.search(Seed.WALK_collection_folder==seed['WALK_collection_folder']))\n",
    "                and seed_main.search(Seed.seed_name==seed['seed_name']))\n",
    "print (seed_exists)\n",
    "seed['TIMESTAMP'] = time.time()\n",
    "if seed_exists:\n",
    "    # print(collections.search(Seed.collection_title.exists()))\n",
    "    sel = seed_main.get(Seed.collection_title==seed['collection_title'])\n",
    "    sil = seed_main.get(Seed.WALK_collection_folder==seed['WALK_collection_folder'])\n",
    "    ssl = seed_main.get(Seed.seed_name==seed['seed_name'])\n",
    "    if sel.eid == sil.eid == ssl.eid:\n",
    "        seed_main.update(seed, eids=[el.eid])\n",
    "        seed_backup.insert(seed)\n",
    "    print (\"updated \"  + seed['collection_title'] + \".\\n\")\n",
    "    print (\"Previous insert added to backup log.\\n\")\n",
    "    Seed_backup = max([(x['collection_title'], x['TIMESTAMP'], x.eid) for x in seed_backup.search(Seed.TIMESTAMP > 0)])\n",
    "    print ('title: \"' + Seed_backup[0] + \"\\ntimestamp: \" + str(Seed_backup[1]) + \"\\neid (aka id): \" + str(Seed_backup[2]))\n",
    "        \n",
    "else:\n",
    "    seed_main.insert(seed)\n",
    "    print (\"inserted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canadian Political Parties and Political Interest Groups\n",
      "has the following seeds:\n",
      "   Cosmopolitan Party of Canada\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "RESULTS = collections.search(Seed.collection_title.exists())\n",
    "for item in RESULTS:\n",
    "    print (item['collection_title'])\n",
    "    seeds = seed_main.search(Seed.collection_title==item['collection_title'])\n",
    "    print (\"has the following seeds:\")\n",
    "    for se in seeds:\n",
    "        \n",
    "        print (textwrap.indent(se['seed_name'], '   '))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Gephi\n",
    "===============\n",
    "\n",
    "The data that includes Gephi-compatible files are available at the [WALK dataverse page](https://dataverse.scholarsportal.info/dataset.xhtml?persistentId=hdl:10864/12040). Each file is in \"gdf\" format which is compatable with a network visualization tool called [Gephi](https://gephi.org/). We are going to use gephi to produce attractive web graphs.\n",
    "\n",
    "* Start by downloading the file and starting Gephi. \n",
    "        > File > Open (or âŒ˜o)\n",
    "        > Change file format to GDF (Guess) *.gdf\n",
    "        > Select the file and open\n",
    "\n",
    "The raw data will probably just be a large set of nodes with no real order to them. Not very helpful to us.\n",
    "\n",
    "\n",
    "<img src=\"img/firstGephi.png\" alt=\"Not very helpful Gephi visualization\" width=\"200\" height=\"200\" />\n",
    "\n",
    "\n",
    "We need to filter them.  A good way is to focus on [strongly connected components](http://www.geeksforgeeks.org/strongly-connected-components/). A component is just a group of nodes (in this case websites) all connected together. *Strongly* connected components are a little different in that everyone in the component will have access to the same information. A good test of a strongly connected component is if you run your finger from node to node, following the direction of each arrow, eventually you will be able to access every node.\n",
    "\n",
    "If we use the statistics menu, gephi will calculate a number of things that are important to us. To create a strongly connected component graph, we need to run the connected components algorithm. \n",
    "\n",
    "<img src=\"img/menu.png\" alt=\"Gephi Statistics Menu\" width=\"200\" height=\"800\" />\n",
    "\n",
    "It will ask us if we want to do this in a directed or undirected fashion. Directed means that we care about the direction of the linkages, so choose that (it should be the default).\n",
    "\n",
    "Now for the hard part.  Out of many thousands of strongly connected components, we need to find the largest ones.\n",
    "\n",
    "Use the top bar to switch to the data laboratory.\n",
    "\n",
    "<img src=\"img/topbar.png\" alt=\"Gephi Top Bar\" width=\"300\" height=\"200\" />\n",
    "\n",
    "\n",
    "There's a light bulb icon at the far right of the spreadsheet.  Click that to limit the number of pages you see.\n",
    "\n",
    "<img src=\"img/lightbulb.png\" alt=\"Gephi Light Bulb\" width=\"300\" height=\"200\" />\n",
    "\n",
    "Unclick all the checks except for strongly connected component.  Then click the label \"Strongly Connected Component\" to sort the list.  \n",
    "\n",
    "Perhaps there is a better way to do this with a regular expression, but we need to find the Strongly Connected Component ID that has a largest number of nodes. Unfortunately, this requires scrolling through the list to find what ID that is.  Fortunately, the largest component is probably so large that as you scroll quickly down, it will be obvious because the ids will be the same for a while.  In my case the number was 13047 (yes, pretty far down) but I found it in less than 2 minutes.\n",
    "\n",
    "<img src=\"img/list.png\" alt=\"Long List Changing\" width=\"200\" height=\"800\" />\n",
    "\n",
    "Now to filter the graph. Go back to \"Overview\" to see the graph.  On the statistics menu, there is a tab called \"filter\" click that.\n",
    "\n",
    "* Click attributes > Equal \n",
    "\n",
    "and select Strongly Connected Component ID. \n",
    "\n",
    "at the bottom, you will see \"value.\" Enter the ID of the component with the large number of units. (Again, mine was 13047). Then click \"filter.\"  The new graph will be much more manageable.\n",
    "\n",
    "<img src=\"img/filtered.png\" alt=\"Filtered Graph\" width=\"200\" height=\"200\" />\n",
    "\n",
    "\n",
    "Now on the left, we can use an algorithm to organize the nodes a little better.  Go to the \"layout\" menu and select the \"Yifan Hu\" algorithm. \n",
    "\n",
    "<img src=\"img/yifan.png\" alt=\"Yifan Hu Menu\" width=\"200\" height=\"800\" />\n",
    "\n",
    "You'll find the layout much better organized.\n",
    "\n",
    "<img src=\"img/yifangraph.png\" alt=\"Yifan Hu Graph\" width=\"200\" height=\"200\" />\n",
    "\n",
    "It's possible that the Yifan Hu is not the best possible layout for your graph.  This is going to depend on what you notice in the graph and what you want to highlight.  Here are some other recommendations:\n",
    "\n",
    "* Force Atlas - This algorithm is better if you have a smaller graph\n",
    "\n",
    "<img src=\"img/fatlas.png\" alt=\"Forced Atlas Graph\" width=\"200\" height=\"200\" />\n",
    "\n",
    "\n",
    "* Circular - This algorithm emphasizes the edges of the graph, as all the nodes are in a circle.\n",
    "\n",
    "<img src=\"img/circular.png\" alt=\"Circular Graph\" width=\"200\" height=\"200\" />\n",
    "\n",
    "\n",
    "* Fruchtermann-Reingold - spaces all the nodes an equal distance apart.\n",
    "\n",
    "<img src=\"img/fruchtermann.png\" alt=\"Fruchtermann-Reingold\" width=\"200\" height=\"200\" />\n",
    "\n",
    "\n",
    "Now the layout is fixed, we can add some color to the graph.  Return to the statistics menu (hit the \"statistics\" tab where you did your filter, and run the \"average degree,\" \"average path length\" and \"modularity\" statistics.\n",
    "\n",
    "Now go to the top left \"partition\" menu.\n",
    "\n",
    "<img src=\"img/partition.png\" alt=\"Partition menu\" width=\"200\" height=\"400\" />\n",
    "\n",
    "Click the \"refresh\" icon to ensure all your new statistics appear.  Then use the select menu to choose \"modularity class\" as a partition.  Gephi will select a number of items based on a modularity (a rough community detection) algorithm.  Then choose a \"sizing\" item to size the nodes based on their degree, betweenness or other value.\n",
    "\n",
    "There are a variety of things you can do with Gephi, that I will leave up to you now. But I do have some suggestions:\n",
    "\n",
    "* Try eliminating some nodes from the analysis that are too obvious (eg. Google, Youtube, Twitter, Facebook).\n",
    "* Use \"spline\" to control for large network (long tail effects), for example when one node has so many more links than everyone else that it's hard to see what's going on.\n",
    "* Try grouping sets of nodes together that have a lot in common (e.g. if there's an NPD.org (french) and an NDP.ca (english) website).\n",
    "\n",
    "This is what I got after a little bit of fiddling with the results:\n",
    "\n",
    "<img src=\"img/cpp.png\" alt=\"Final Product\" width=\"800\" height=\"800\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
